"""
æ¨¡å‹è®­ç»ƒå™¨ - ä½¿ç”¨æœ¬åœ°æ¨¡å‹è®­ç»ƒå®‰å…¨é˜²æŠ¤æ 
ä¸¥æ ¼æŒ‰ç…§é…ç½®æ–‡ä»¶ä¸­çš„å‚æ•°è¿›è¡Œè®­ç»ƒ
"""

import torch
import json
import yaml
from pathlib import Path
import logging
from datetime import datetime
import os

class æ¨¡å‹è®­ç»ƒå™¨:
    def __init__(self):
        # è®¾ç½®è·¯å¾„
        self.å½“å‰ç›®å½• = Path(__file__).parent.parent
        self.é…ç½®è·¯å¾„ = self.å½“å‰ç›®å½• / "é…ç½®æ–‡ä»¶" / "é…ç½®.yaml"
        self.è®­ç»ƒæ•°æ®è·¯å¾„ = self.å½“å‰ç›®å½• / "æ•°æ®" / "å¤„ç†æ•°æ®" / "è®­ç»ƒé›†.json"
        self.æ¨¡å‹ä¿å­˜è·¯å¾„ = self.å½“å‰ç›®å½• / "æ¨¡å‹æ–‡ä»¶"

        # âœ… ä¿®å¤ï¼šå…ˆè®¾ç½®æ—¥å¿—ï¼
        self.è®¾ç½®æ—¥å¿—()

        # âœ… ç„¶ååŠ è½½é…ç½®ï¼ˆæ­¤æ—¶self.æ—¥å¿—å·²å­˜åœ¨ï¼‰
        self.é…ç½® = self.åŠ è½½é…ç½®()

        # ç¡®ä¿ç›®å½•å­˜åœ¨
        self.æ¨¡å‹ä¿å­˜è·¯å¾„.mkdir(parents=True, exist_ok=True)

        # éªŒè¯é…ç½®
        if self.é…ç½®:
            self.éªŒè¯é…ç½®()

    def è®¾ç½®æ—¥å¿—(self):
        """è®¾ç½®è®­ç»ƒæ—¥å¿—"""
        æ—¥å¿—ç›®å½• = self.å½“å‰ç›®å½• / "æ—¥å¿—æ–‡ä»¶"
        æ—¥å¿—ç›®å½•.mkdir(parents=True, exist_ok=True)

        æ—¥å¿—æ–‡ä»¶ = æ—¥å¿—ç›®å½• / f"è®­ç»ƒæ—¥å¿—_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"

        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(æ—¥å¿—æ–‡ä»¶, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.æ—¥å¿— = logging.getLogger(__name__)

    def åŠ è½½é…ç½®(self):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            if not self.é…ç½®è·¯å¾„.exists():
                self.æ—¥å¿—.error(f"æ‰¾ä¸åˆ°é…ç½®æ–‡ä»¶: {self.é…ç½®è·¯å¾„}")
                return {}

            with open(self.é…ç½®è·¯å¾„, 'r', encoding='utf-8') as f:
                é…ç½® = yaml.safe_load(f)

            self.æ—¥å¿—.info(f"âœ… é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ")
            return é…ç½®

        except Exception as e:
            self.æ—¥å¿—.error(f"åŠ è½½é…ç½®å¤±è´¥: {e}")
            return {}

    def éªŒè¯é…ç½®(self):
        """éªŒè¯é…ç½®å‚æ•°çš„å®Œæ•´æ€§"""
        if not self.é…ç½®:
            self.æ—¥å¿—.error("é…ç½®ä¸ºç©º")
            return False

        # æ£€æŸ¥å…³é”®é…ç½®
        å¿…è¦é…ç½® = {
            'æ¨¡å‹': ['åŸºç¡€æ¨¡å‹', 'LoRAå‚æ•°r', 'LoRAå‚æ•°alpha', 'LoRAå±‚'],
            'è®­ç»ƒ': ['å­¦ä¹ ç‡', 'è®­ç»ƒè½®æ•°', 'æ‰¹æ¬¡å¤§å°', 'æ¢¯åº¦ç´¯ç§¯', 'çƒ­èº«æ­¥æ•°', 'æ—¥å¿—é—´éš”', 'ä¿å­˜é—´éš”']
        }

        for æ¨¡å—, å‚æ•°åˆ—è¡¨ in å¿…è¦é…ç½®.items():
            if æ¨¡å— not in self.é…ç½®:
                self.æ—¥å¿—.error(f"âŒ é…ç½®ç¼ºå°‘æ¨¡å—: {æ¨¡å—}")
                return False

            for å‚æ•° in å‚æ•°åˆ—è¡¨:
                if å‚æ•° not in self.é…ç½®[æ¨¡å—]:
                    self.æ—¥å¿—.warning(f"âš ï¸  {æ¨¡å—}.{å‚æ•°} æœªè®¾ç½®ï¼Œä½¿ç”¨é»˜è®¤å€¼")

        return True

    def å¼€å§‹è®­ç»ƒ(self):
        """å¼€å§‹æ¨¡å‹è®­ç»ƒæµç¨‹"""
        print("="*60)
        print("ğŸ¯ å¼€å§‹æ¨¡å‹è®­ç»ƒ - æœ¬åœ°æ¨¡å‹ç‰ˆæœ¬")
        print("="*60)

        # 1. æ£€æŸ¥è®­ç»ƒæ•°æ®
        if not self.æ£€æŸ¥è®­ç»ƒæ•°æ®():
            return False

        # 2. æ˜¾ç¤ºé…ç½®ä¿¡æ¯
        self.æ˜¾ç¤ºé…ç½®ä¿¡æ¯()

        # 3. è¯¢é—®æ˜¯å¦å¼€å§‹è®­ç»ƒ
        ç¡®è®¤ = input("\nâš ï¸  æ˜¯å¦å¼€å§‹è®­ç»ƒ? (y/N): ").strip().lower()
        if ç¡®è®¤ != 'y':
            print("è®­ç»ƒå·²å–æ¶ˆ")
            return False

        # 4. æ‰§è¡Œè®­ç»ƒ
        return self.æ‰§è¡Œè®­ç»ƒ()

    def æ£€æŸ¥è®­ç»ƒæ•°æ®(self):
        """æ£€æŸ¥è®­ç»ƒæ•°æ®æ˜¯å¦å­˜åœ¨"""
        if not self.è®­ç»ƒæ•°æ®è·¯å¾„.exists():
            print("âŒ æ‰¾ä¸åˆ°è®­ç»ƒæ•°æ®")
            print(f"è¯·å…ˆè¿è¡Œ'å¤„ç†æ•°æ®'ç”Ÿæˆè®­ç»ƒé›†")
            print(f"é¢„æœŸè·¯å¾„: {self.è®­ç»ƒæ•°æ®è·¯å¾„}")
            return False

        try:
            with open(self.è®­ç»ƒæ•°æ®è·¯å¾„, 'r', encoding='utf-8') as f:
                è®­ç»ƒæ•°æ® = json.load(f)

            if not è®­ç»ƒæ•°æ®:
                print("âŒ è®­ç»ƒæ•°æ®ä¸ºç©º")
                return False

            print(f"âœ… æ‰¾åˆ°è®­ç»ƒæ•°æ®: {len(è®­ç»ƒæ•°æ®)} ä¸ªæ ·æœ¬")
            return True

        except Exception as e:
            print(f"âŒ åŠ è½½è®­ç»ƒæ•°æ®å¤±è´¥: {e}")
            return False

    def æ˜¾ç¤ºé…ç½®ä¿¡æ¯(self):
        """æ˜¾ç¤ºå½“å‰é…ç½®ä¿¡æ¯"""
        print("\nğŸ“Š å½“å‰é…ç½®:")
        print("="*40)

        # æ¨¡å‹é…ç½®
        print("ğŸ¤– æ¨¡å‹é…ç½®:")
        if 'æ¨¡å‹' in self.é…ç½®:
            æ¨¡å‹é…ç½® = self.é…ç½®['æ¨¡å‹']
            print(f"   åŸºç¡€æ¨¡å‹: {æ¨¡å‹é…ç½®.get('åŸºç¡€æ¨¡å‹', 'æœªè®¾ç½®')}")
            print(f"   LoRAå‚æ•°r: {æ¨¡å‹é…ç½®.get('LoRAå‚æ•°r', 'æœªè®¾ç½®')}")
            print(f"   LoRAå‚æ•°alpha: {æ¨¡å‹é…ç½®.get('LoRAå‚æ•°alpha', 'æœªè®¾ç½®')}")
            print(f"   dropoutç‡: {æ¨¡å‹é…ç½®.get('dropoutç‡', 'æœªè®¾ç½®')}")
            print(f"   LoRAå±‚: {æ¨¡å‹é…ç½®.get('LoRAå±‚', 'æœªè®¾ç½®')}")

        # è®­ç»ƒé…ç½®
        print("\nğŸ¯ è®­ç»ƒé…ç½®:")
        if 'è®­ç»ƒ' in self.é…ç½®:
            è®­ç»ƒé…ç½® = self.é…ç½®['è®­ç»ƒ']
            print(f"   å­¦ä¹ ç‡: {è®­ç»ƒé…ç½®.get('å­¦ä¹ ç‡', 'æœªè®¾ç½®')}")
            print(f"   è®­ç»ƒè½®æ•°: {è®­ç»ƒé…ç½®.get('è®­ç»ƒè½®æ•°', 'æœªè®¾ç½®')}")
            print(f"   æ‰¹æ¬¡å¤§å°: {è®­ç»ƒé…ç½®.get('æ‰¹æ¬¡å¤§å°', 'æœªè®¾ç½®')}")
            print(f"   æ¢¯åº¦ç´¯ç§¯: {è®­ç»ƒé…ç½®.get('æ¢¯åº¦ç´¯ç§¯', 'æœªè®¾ç½®')}")
            print(f"   çƒ­èº«æ­¥æ•°: {è®­ç»ƒé…ç½®.get('çƒ­èº«æ­¥æ•°', 'æœªè®¾ç½®')}")
            print(f"   æ—¥å¿—é—´éš”: {è®­ç»ƒé…ç½®.get('æ—¥å¿—é—´éš”', 'æœªè®¾ç½®')}")
            print(f"   ä¿å­˜é—´éš”: {è®­ç»ƒé…ç½®.get('ä¿å­˜é—´éš”', 'æœªè®¾ç½®')}")

        # æ•°æ®ç»Ÿè®¡
        try:
            with open(self.è®­ç»ƒæ•°æ®è·¯å¾„, 'r', encoding='utf-8') as f:
                è®­ç»ƒæ•°æ® = json.load(f)

            å±é™©æ ·æœ¬ = sum(1 for æ•°æ® in è®­ç»ƒæ•°æ® if æ•°æ®.get('æ ‡ç­¾') == 1)
            å®‰å…¨æ ·æœ¬ = sum(1 for æ•°æ® in è®­ç»ƒ_data if æ•°æ®.get('æ ‡ç­¾') == 0)

            print("\nğŸ“ˆ æ•°æ®ç»Ÿè®¡:")
            print(f"   æ€»æ ·æœ¬æ•°: {len(è®­ç»ƒæ•°æ®)}")
            print(f"   å±é™©æ ·æœ¬: {å±é™©æ ·æœ¬}")
            print(f"   å®‰å…¨æ ·æœ¬: {å®‰å…¨æ ·æœ¬}")
            if è®­ç»ƒæ•°æ®:
                print(f"   å±é™©æ¯”ä¾‹: {å±é™©æ ·æœ¬/len(è®­ç»ƒ_data)*100:.1f}%")
        except:
            print("\nğŸ“ˆ æ•°æ®ç»Ÿè®¡: æ— æ³•åŠ è½½æ•°æ®")

        print("="*40)

    def æ‰§è¡Œè®­ç»ƒ(self):
        """æ‰§è¡Œæ¨¡å‹è®­ç»ƒ - ä¸¥æ ¼ä½¿ç”¨æœ¬åœ°æ¨¡å‹"""
        print("\n" + "="*60)
        print("ğŸš€ å¼€å§‹æ¨¡å‹è®­ç»ƒ (æœ¬åœ°æ¨¡å‹)")
        print("="*60)

        try:
            # 1. è·å–æœ¬åœ°æ¨¡å‹è·¯å¾„
            model_config = self.é…ç½®.get('æ¨¡å‹', {})
            åŸºç¡€æ¨¡å‹é…ç½® = model_config.get('åŸºç¡€æ¨¡å‹', 'Qwen/Qwen2.5-0.5B-Instruct')

            print(f"ğŸ“ é…ç½®ä¸­çš„æ¨¡å‹è·¯å¾„: {åŸºç¡€æ¨¡å‹é…ç½®}")

            # åˆ¤æ–­æ˜¯å¦æ˜¯æœ¬åœ°è·¯å¾„
            æœ¬åœ°æ¨¡å‹è·¯å¾„ = None

            # æ£€æŸ¥æ˜¯å¦æ˜¯æœ¬åœ°è·¯å¾„
            if os.path.exists(åŸºç¡€æ¨¡å‹é…ç½®):
                æœ¬åœ°æ¨¡å‹è·¯å¾„ = åŸºç¡€æ¨¡å‹é…ç½®
                print(f"âœ… æ‰¾åˆ°æœ¬åœ°æ¨¡å‹ (ç»å¯¹è·¯å¾„): {æœ¬åœ°æ¨¡å‹è·¯å¾„}")
            elif os.path.exists(os.path.join(self.å½“å‰ç›®å½•, åŸºç¡€æ¨¡å‹é…ç½®)):
                æœ¬åœ°æ¨¡å‹è·¯å¾„ = os.path.join(self.å½“å‰ç›®å½•, åŸºç¡€æ¨¡å‹é…ç½®)
                print(f"âœ… æ‰¾åˆ°æœ¬åœ°æ¨¡å‹ (ç›¸å¯¹è·¯å¾„): {æœ¬åœ°æ¨¡å‹è·¯å¾„}")
            else:
                # å°è¯•åœ¨æ¨¡å‹æ–‡ä»¶ç›®å½•ä¸­æŸ¥æ‰¾
                æ¨¡å‹ç›®å½• = self.å½“å‰ç›®å½• / "æ¨¡å‹æ–‡ä»¶"
                if æ¨¡å‹ç›®å½•.exists():
                    å¯èƒ½è·¯å¾„ = list(æ¨¡å‹ç›®å½•.rglob("*"))
                    if å¯èƒ½è·¯å¾„:
                        for è·¯å¾„ in å¯èƒ½è·¯å¾„:
                            if è·¯å¾„.is_dir() and any(è·¯å¾„.glob("*.safetensors")) or any(è·¯å¾„.glob("*.bin")):
                                æœ¬åœ°æ¨¡å‹è·¯å¾„ = str(è·¯å¾„)
                                print(f"âœ… åœ¨æ¨¡å‹æ–‡ä»¶ç›®å½•ä¸­æ‰¾åˆ°: {æœ¬åœ°æ¨¡å‹è·¯å¾„}")
                                break

            if æœ¬åœ°æ¨¡å‹è·¯å¾„ is None:
                print("âŒ æ‰¾ä¸åˆ°æœ¬åœ°æ¨¡å‹ï¼Œè¯·ç¡®ä¿æ¨¡å‹æ–‡ä»¶å·²ä¸‹è½½åˆ°ä»¥ä¸‹ä½ç½®ä¹‹ä¸€:")
                print(f"   1. {åŸºç¡€æ¨¡å‹é…ç½®}")
                print(f"   2. {os.path.join(self.å½“å‰ç›®å½•, åŸºç¡€æ¨¡å‹é…ç½®)}")
                print(f"   3. {self.å½“å‰ç›®å½• / 'æ¨¡å‹æ–‡ä»¶'}")
                return False

            print(f"âœ… ä½¿ç”¨æœ¬åœ°æ¨¡å‹: {æœ¬åœ°æ¨¡å‹è·¯å¾„}")

            # 2. åŠ è½½æœ¬åœ°æ¨¡å‹å’Œåˆ†è¯å™¨
            print("\n1ï¸âƒ£ åŠ è½½æœ¬åœ°æ¨¡å‹å’Œåˆ†è¯å™¨...")
            from transformers import AutoModelForCausalLM, AutoTokenizer

            try:
                # å¼ºåˆ¶åªä½¿ç”¨æœ¬åœ°æ–‡ä»¶
                tokenizer = AutoTokenizer.from_pretrained(
                    æœ¬åœ°æ¨¡å‹è·¯å¾„,
                    local_files_only=True,
                    trust_remote_code=True
                )

                if tokenizer.pad_token is None:
                    tokenizer.pad_token = tokenizer.eos_token

                print("   âœ… åˆ†è¯å™¨åŠ è½½æˆåŠŸ")

            except Exception as e:
                print(f"   âŒ åˆ†è¯å™¨åŠ è½½å¤±è´¥: {e}")
                print("   å°è¯•ä½¿ç”¨åŸºç¡€æ¨¡å‹åç§°åŠ è½½...")
                tokenizer = AutoTokenizer.from_pretrained(åŸºç¡€æ¨¡å‹é…ç½®)
                if tokenizer.pad_token is None:
                    tokenizer.pad_token = tokenizer.eos_token

            # åŠ è½½æ¨¡å‹
            device = "cuda" if torch.cuda.is_available() else "cpu"
            print(f"   ä½¿ç”¨è®¾å¤‡: {device}")

            try:
                model = AutoModelForCausalLM.from_pretrained(
                    æœ¬åœ°æ¨¡å‹è·¯å¾„,
                    torch_dtype=torch.float16 if device == "cuda" else torch.float32,
                    device_map="auto" if device == "cuda" else None,
                    local_files_only=True,
                    trust_remote_code=True
                )
                print("   âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
            except Exception as e:
                print(f"   âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                return False

            # 3. é…ç½®LoRA
            print("\n2ï¸âƒ£ é…ç½®LoRA...")
            from peft import LoraConfig, get_peft_model, TaskType

            lora_r = model_config.get('LoRAå‚æ•°r', 16)
            lora_alpha = model_config.get('LoRAå‚æ•°alpha', 32)
            lora_dropout = model_config.get('dropoutç‡', 0.1)
            target_modules = model_config.get('LoRAå±‚', ["q_proj", "k_proj", "v_proj", "o_proj"])

            print(f"   LoRAå‚æ•° r={lora_r}, alpha={lora_alpha}, dropout={lora_dropout}")

            lora_config = LoraConfig(
                task_type=TaskType.CAUSAL_LM,
                r=lora_r,
                lora_alpha=lora_alpha,
                lora_dropout=lora_dropout,
                target_modules=target_modules,
                bias="none"
            )

            model = get_peft_model(model, lora_config)
            model.print_trainable_parameters()

            # 4. å‡†å¤‡è®­ç»ƒæ•°æ®
            print("\n3ï¸âƒ£ å‡†å¤‡è®­ç»ƒæ•°æ®...")
            with open(self.è®­ç»ƒæ•°æ®è·¯å¾„, 'r', encoding='utf-8') as f:
                è®­ç»ƒæ•°æ® = json.load(f)

            # è½¬æ¢æ•°æ®æ ¼å¼
            def æ ¼å¼åŒ–æ ·æœ¬(æ ·æœ¬):
                æç¤ºè¯ = æ ·æœ¬.get('æç¤ºè¯', '')
                æ¨ç†è¿‡ç¨‹ = æ ·æœ¬.get('æ¨ç†è¿‡ç¨‹', '')
                return f"æç¤ºè¯: {æç¤ºè¯}\nå®‰å…¨åˆ†æ: {æ¨ç†è¿‡ç¨‹}"

            è®­ç»ƒæ–‡æœ¬ = [æ ¼å¼åŒ–æ ·æœ¬(æ ·æœ¬) for æ ·æœ¬ in è®­ç»ƒæ•°æ®]

            # åˆ›å»ºæ•°æ®é›†
            from datasets import Dataset
            dataset = Dataset.from_dict({"text": è®­ç»ƒæ–‡æœ¬})

            def é¢„å¤„ç†å‡½æ•°(æ ·æœ¬):
                # ä½¿ç”¨ return_tensors="pt" è¿”å›PyTorchå¼ é‡
                ç¼–ç ç»“æœ = tokenizer(
                    æ ·æœ¬["text"],
                    truncation=True,
                    padding="max_length",
                    max_length=512,
                    return_tensors="pt"  # âœ… å…³é”®ï¼šè¿”å›å¼ é‡æ ¼å¼
                )

                # ä»æ‰¹å¤„ç†ä¸­æå–å•ä¸ªæ ·æœ¬
                return {key: val[0] for key, val in ç¼–ç ç»“æœ.items()}

            å¤„ç†åæ•°æ®é›† = dataset.map(é¢„å¤„ç†å‡½æ•°, batched=False)  # âœ… å•æ ·æœ¬å¤„ç†

            # 5. è®¾ç½®è®­ç»ƒå‚æ•°ï¼ˆä¸¥æ ¼æŒ‰ç…§é…ç½®ï¼‰
            print("\n4ï¸âƒ£ é…ç½®è®­ç»ƒå‚æ•°...")
            from transformers import TrainingArguments

            è®­ç»ƒé…ç½® = self.é…ç½®.get('è®­ç»ƒ', {})

            training_args = TrainingArguments(
                output_dir=str(self.æ¨¡å‹ä¿å­˜è·¯å¾„ / "è®­ç»ƒè¾“å‡º"),
                num_train_epochs=è®­ç»ƒé…ç½®.get('è®­ç»ƒè½®æ•°', 3),
                per_device_train_batch_size=è®­ç»ƒé…ç½®.get('æ‰¹æ¬¡å¤§å°', 4),
                gradient_accumulation_steps=è®­ç»ƒé…ç½®.get('æ¢¯åº¦ç´¯ç§¯', 4),
                warmup_steps=è®­ç»ƒé…ç½®.get('çƒ­èº«æ­¥æ•°', 100),
                logging_steps=è®­ç»ƒé…ç½®.get('æ—¥å¿—é—´éš”', 10),
                save_steps=è®­ç»ƒé…ç½®.get('ä¿å­˜é—´éš”', 100),
                learning_rate=è®­ç»ƒé…ç½®.get('å­¦ä¹ ç‡', 0.0002),
                fp16=torch.cuda.is_available(),
                logging_dir=str(self.å½“å‰ç›®å½• / "æ—¥å¿—æ–‡ä»¶" / "è®­ç»ƒæ—¥å¿—"),
                report_to="none",
                remove_unused_columns=False,
                save_total_limit=2,
                load_best_model_at_end=False,
                metric_for_best_model="loss",
                greater_is_better=False
            )

            # 6. åˆ›å»ºè®­ç»ƒå™¨
            from transformers import Trainer, DataCollatorForSeq2Seq

            data_collator = DataCollatorForSeq2Seq(tokenizer, pad_to_multiple_of=8)

            trainer = Trainer(
                model=model,
                args=training_args,
                train_dataset=å¤„ç†åæ•°æ®é›†,
                data_collator=data_collator,
                tokenizer=tokenizer
            )

            # 7. å¼€å§‹è®­ç»ƒ
            print("\n5ï¸âƒ£ å¼€å§‹è®­ç»ƒ...")
            print(f"   è®­ç»ƒè½®æ•°: {è®­ç»ƒé…ç½®.get('è®­ç»ƒè½®æ•°', 3)}")
            print(f"   å­¦ä¹ ç‡: {è®­ç»ƒé…ç½®.get('å­¦ä¹ ç‡', 0.0002)}")
            print(f"   æ‰¹æ¬¡å¤§å°: {è®­ç»ƒé…ç½®.get('æ‰¹æ¬¡å¤§å°', 4)}")
            print(f"   æ¢¯åº¦ç´¯ç§¯: {è®­ç»ƒé…ç½®.get('æ¢¯åº¦ç´¯ç§¯', 4)}")
            print(f"   æ€»è®­ç»ƒæ­¥æ•°: {len(å¤„ç†åæ•°æ®é›†) // è®­ç»ƒé…ç½®.get('æ‰¹æ¬¡å¤§å°', 4) * è®­ç»ƒé…ç½®.get('è®­ç»ƒè½®æ•°', 3)}")
            print("-"*40)

            trainer.train()

            # 8. ä¿å­˜æ¨¡å‹
            print("\n6ï¸âƒ£ ä¿å­˜æ¨¡å‹...")
            æ¨¡å‹ä¿å­˜åç§° = f"å®‰å…¨é˜²æŠ¤æ æ¨¡å‹_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            ä¿å­˜è·¯å¾„ = self.æ¨¡å‹ä¿å­˜è·¯å¾„ / æ¨¡å‹ä¿å­˜åç§°
            ä¿å­˜è·¯å¾„.mkdir(parents=True, exist_ok=True)

            # ä¿å­˜æ¨¡å‹
            trainer.save_model(str(ä¿å­˜è·¯å¾„))
            tokenizer.save_pretrained(str(ä¿å­˜è·¯å¾„))

            # ä¿å­˜æ¨¡å‹ä¿¡æ¯
            æ¨¡å‹ä¿¡æ¯ = {
                "æ¨¡å‹åç§°": æ¨¡å‹ä¿å­˜åç§°,
                "åŸºç¡€æ¨¡å‹": åŸºç¡€æ¨¡å‹é…ç½®,
                "æœ¬åœ°è·¯å¾„": æœ¬åœ°æ¨¡å‹è·¯å¾„,
                "è®­ç»ƒæ—¶é—´": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "æ ·æœ¬æ•°é‡": len(è®­ç»ƒæ•°æ®),
                "è®­ç»ƒå‚æ•°": {
                    "å­¦ä¹ ç‡": è®­ç»ƒé…ç½®.get('å­¦ä¹ ç‡', 0.0002),
                    "è®­ç»ƒè½®æ•°": è®­ç»ƒé…ç½®.get('è®­ç»ƒè½®æ•°', 3),
                    "æ‰¹æ¬¡å¤§å°": è®­ç»ƒé…ç½®.get('æ‰¹æ¬¡å¤§å°', 4),
                    "æ¢¯åº¦ç´¯ç§¯": è®­ç»ƒé…ç½®.get('æ¢¯åº¦ç´¯ç§¯', 4),
                    "LoRA_rank": model_config.get('LoRAå‚æ•°r', 16),
                    "LoRA_alpha": model_config.get('LoRAå‚æ•°alpha', 32)
                },
                "æ¨¡å‹æ–‡ä»¶": []
            }

            # æ·»åŠ å®é™…å­˜åœ¨çš„æ–‡ä»¶
            for æ–‡ä»¶ in ä¿å­˜è·¯å¾„.glob("*"):
                æ¨¡å‹ä¿¡æ¯["æ¨¡å‹æ–‡ä»¶"].append(æ–‡ä»¶.name)

            æ¨¡å‹ä¿¡æ¯æ–‡ä»¶ = ä¿å­˜è·¯å¾„ / "æ¨¡å‹ä¿¡æ¯.json"
            with open(æ¨¡å‹ä¿¡æ¯æ–‡ä»¶, 'w', encoding='utf-8') as f:
                json.dump(æ¨¡å‹ä¿¡æ¯, f, ensure_ascii=False, indent=2)

            # æ›´æ–°ä¸»æ¨¡å‹ä¿¡æ¯æ–‡ä»¶
            with open(self.æ¨¡å‹ä¿å­˜è·¯å¾„ / "æ¨¡å‹ä¿¡æ¯.json", 'w', encoding='utf-8') as f:
                json.dump(æ¨¡å‹ä¿¡æ¯, f, ensure_ascii=False, indent=2)

            print("\n" + "="*60)
            print("âœ… æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
            print("="*60)
            print(f"\nğŸ“ æ¨¡å‹å·²ä¿å­˜åˆ°: {ä¿å­˜è·¯å¾„}")
            print("ğŸ“„ æ¨¡å‹æ–‡ä»¶åˆ—è¡¨:")
            for æ–‡ä»¶ in ä¿å­˜_path.glob("*"):
                print(f"  - {æ–‡ä»¶.name}")

            return True

        except ImportError as e:
            print(f"\nâŒ ç¼ºå°‘å¿…è¦çš„ä¾èµ–åº“: {e}")
            print("è¯·å®‰è£…ä¾èµ–: pip install transformers peft datasets")
            return False
        except Exception as e:
            print(f"\nâŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return False

# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    print("ğŸ§ª æµ‹è¯•æ¨¡å‹è®­ç»ƒå™¨ (æœ¬åœ°æ¨¡å‹ç‰ˆæœ¬)...")

    try:
        è®­ç»ƒå™¨ = æ¨¡å‹è®­ç»ƒå™¨()

        # æ˜¾ç¤ºé…ç½®
        è®­ç»ƒå™¨.æ˜¾ç¤ºé…ç½®ä¿¡æ¯()

        # è¯¢é—®æ˜¯å¦æµ‹è¯•è®­ç»ƒ
        æµ‹è¯• = input("\næ˜¯å¦æµ‹è¯•è®­ç»ƒæµç¨‹? (y/N): ").strip().lower()
        if æµ‹è¯• == 'y':
            è®­ç»ƒå™¨.å¼€å§‹è®­ç»ƒ()
        else:
            print("æµ‹è¯•å®Œæˆï¼Œæœªå¼€å§‹è®­ç»ƒ")
    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()